import torch
import torch.nn as nn
import torch.optim as optim
import gymnasium as gym
import numpy as np
from collections import defaultdict
from scipy.integrate import solve_ivp
import time
from matplotlib import pyplot as plt


# Define the Actor-Critic Network
class ActorCritic(nn.Module):
    def __init__(self, env, hidden_size=40):
        super(ActorCritic, self).__init__()
        num_inputs = env.observation_space.shape[0]
        num_actions = env.action_space.n

        # Define your layers here:
        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)
        self.critic_linear2 = nn.Linear(hidden_size, 1)
        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)
        self.actor_linear2 = nn.Linear(hidden_size, num_actions)
    
    def actor(self, state, return_logp=False):
        hidden = torch.tanh(self.actor_linear1(state))
        h = self.actor_linear2(hidden)
        h = h - torch.max(h, dim=1, keepdim=True)[0]
        logp = h - torch.log(torch.sum(torch.exp(h), dim=1, keepdim=True))
        if return_logp:
            return logp
        else:
            return torch.exp(logp)
    
    def critic(self, state):
        hidden = torch.tanh(self.critic_linear1(state))
        return self.critic_linear2(hidden)[:, 0]
    
    def forward(self, state):
        return self.critic(state), self.actor(state)


# Define the Unbalanced Disk Environment (your custom gym environment)
class UnbalancedDisk(gym.Env):
    def __init__(self, nvec=40, umax=3., dt=0.025, render_mode='human'):
        self.omega0 = 11.339846957335382
        self.delta_th = 0
        self.gamma = 1.3328339309394384
        self.Ku = 28.136158407237073
        self.Fc = 6.062729509386865
        self.coulomb_omega = 0.001

        self.umax = umax
        self.dt = dt
        self.action_space = gym.spaces.Discrete(7)
        self.observation_space = gym.spaces.Box(low=np.array([-np.pi, -40], dtype=np.float32),
                                                 high=np.array([np.pi, 40], dtype=np.float32), shape=(2,))

        self.reward_fun = lambda self: (
            1000 * np.cos(self.th - np.pi) + 10 * np.cos(self.th - np.pi) * (self.dt / 0.025) +
            100 * abs(np.sin(self.th / 2)) + 0.5 * (1 - np.cos(self.th)) * abs(self.omega) -
            0.001 * self.u ** 2 - 0.1 * abs(self.delta_th) if abs(self.delta_th) < np.pi / 2 else 0 - 
            50 * abs(self.omega) if abs(self.delta_th) >= np.pi - 0.1415 else 0
        )

        self.render_mode = render_mode
        self.viewer = None
        self.reset()

    def step(self, action):
        self.u = [-3, -1, -0.5, 0, 0.5, 1, 3][action]
        self.u = np.clip(self.u, -self.umax, self.umax)

        def f(t, y):
            th, omega = y
            dthdt = omega
            friction = self.gamma * omega + self.Fc * np.tanh(omega / self.coulomb_omega)
            domegadt = -self.omega0 ** 2 * np.sin(th + self.delta_th) - friction + self.Ku * self.u
            return np.array([dthdt, domegadt])

        sol = solve_ivp(f, [0, self.dt], [self.th, self.omega])
        th, self.omega = sol.y[:, -1]
        self.delta_th = np.arctan2(np.sin(th - self.th), np.cos(th - self.th))
        self.th = th
        self.costh = -np.cos(th)

        reward = self.reward_fun(self)
        terminated = False
        return self.get_obs(), reward, terminated, False, [self.th, self.omega, self.delta_th]

    def reset(self):
        self.th = np.random.normal(loc=0, scale=0.001)
        self.omega = np.random.normal(loc=0, scale=0.001)
        self.u = 0
        self.delta_th = 0
        return self.get_obs(), {}

    def get_obs(self):
        self.th_noise = self.th + np.random.normal(loc=0, scale=0.001)
        self.omega_noise = self.omega + np.random.normal(loc=0, scale=0.001)
        return np.array([self.th_noise, self.omega_noise])

    def render(self):
        import pygame
        from pygame import gfxdraw

        screen_width = 500
        screen_height = 500

        th = self.th
        omega = self.omega

        if self.viewer is None:
            pygame.init()
            pygame.display.init()
            self.viewer = pygame.display.set_mode((screen_width, screen_height))

        self.surf = pygame.Surface((screen_width, screen_height))
        self.surf.fill((255, 255, 255))

        gfxdraw.filled_circle(self.surf, screen_width // 2, screen_height // 2,
                              int(screen_width / 2 * 0.65 * 1.3), (32, 60, 92))
        gfxdraw.filled_circle(self.surf, screen_width // 2, screen_height // 2,
                              int(screen_width / 2 * 0.06 * 1.3), (132, 132, 126))

        r = screen_width // 2 * 0.40 * 1.3
        gfxdraw.filled_circle(self.surf, int(screen_width // 2 - np.sin(th) * r),
                              int(screen_height // 2 - np.cos(th) * r), int(screen_width / 2 * 0.22 * 1.3),
                              (155, 140, 108))
        gfxdraw.filled_circle(self.surf, int(screen_width // 2 - np.sin(th) * r),
                              int(screen_height // 2 - np.cos(th) * r), int(screen_width / 2 * 0.22 / 8 * 1.3),
                              (71, 63, 48))

        fname = "clockwise.png"
        self.arrow = pygame.image.load(fname)
        if self.u:
            if isinstance(self.u, (np.ndarray, list)):
                if self.u.ndim == 1:
                    u = self.u[0]
                elif self.u.ndim == 0:
                    u = self.u
                else:
                    raise ValueError(f'u={u} is not the correct shape')
            else:
                u = self.u
            arrow_size = abs(float(u) / self.umax * screen_height) * 0.25
            Z = (arrow_size, arrow_size)
            arrow_rot = pygame.transform.scale(self.arrow, Z)
            if self.u < 0:
                arrow_rot = pygame.transform.flip(arrow_rot, True, False)

        self.surf = pygame.transform.flip(self.surf, False, True)
        self.viewer.blit(self.surf, (0, 0))
        if self.u:
            self.viewer.blit(arrow_rot, (screen_width // 2 - arrow_size // 2, screen_height // 2 - arrow_size // 2))

        if self.render_mode == "human":
            pygame.event.pump()
            pygame.display.flip()

        return True

    def close(self):
        if self.viewer is not None:
            import pygame

            pygame.display.quit()
            pygame.quit()
            self.isopen = False
            self.viewer = None


# A2C Training function
def A2C_rollout(actor_crit, optimizer, env, alpha_actor=0.5, alpha_entropy=0.5, gamma=0.98, 
                N_iterations=21, N_rollout=20, N_epochs=10, batch_size=32, N_evals=10):
    best = -float('inf')
    torch.save(actor_crit.state_dict(), 'actor-crit-checkpoint')
    try:
        for iteration in range(N_iterations):
            print(f'Rollout iteration {iteration}')
            
            # 2. Collect Rollout Data
            Start_state, Actions, Rewards, End_state, Terminal = rollout(actor_crit, env, N_rollout=N_rollout)
            
            # Convert data to tensors
            convert = lambda x: [torch.tensor(xi, dtype=torch.float32) for xi in x]
            Start_state, Rewards, End_state, Terminal = convert([Start_state, Rewards, End_state, Terminal])
            Actions = Actions.astype(int)

            print('Starting training on rollout information...')
            for epoch in range(N_epochs):
                for i in range(batch_size, len(Start_state) + 1, batch_size):
                    # Create mini-batches
                    Start_state_batch, Actions_batch, Rewards_batch, End_state_batch, Terminal_batch = \
                        [d[i-batch_size:i] for d in [Start_state, Actions, Rewards, End_state, Terminal]]
                    
                    # Advantage estimation
                    Vnow = actor_crit.critic(Start_state_batch)
                    Vnext = actor_crit.critic(End_state_batch)
                    A = Rewards_batch + gamma * Vnext * (1 - Terminal_batch) - Vnow

                    # Calculate log probabilities for selected actions
                    logp = actor_crit.actor(Start_state_batch, return_logp=True)
                    logp_cur = logp[np.arange(batch_size), Actions_batch]
                    p = torch.exp(logp)  # Probabilities of all actions
                    p_cur = torch.exp(logp_cur)  # Probability of selected actions
                    
                    # Loss calculations
                    L_value_function = torch.mean(A**2)  # Critic loss (MSE)
                    L_policy = -(A.detach() * logp_cur).mean()  # Actor loss (Policy gradient)
                    L_entropy = -torch.mean((p * logp).sum(1))  # Entropy loss
                    
                    # Total loss
                    Loss = L_value_function + alpha_actor * L_policy + alpha_entropy * L_entropy
                    
                    optimizer.zero_grad()
                    Loss.backward()
                    optimizer.step()
                
                print(f'Iteration={iteration} Epoch={epoch} Average Reward per episode:', np.mean([eval_actor(actor_crit, env) for _ in range(N_evals)]))
                print(f'\t Value loss:  {L_value_function.item()}')
                print(f'\t Policy loss: {L_policy.item()}')
                print(f'\t Entropy:     {-L_entropy.item()}')

                # Save the best model
                score = np.mean([eval_actor(actor_crit, env) for _ in range(N_evals)])
                if score > best:
                    best = score
                    print(f'################################# \nNew best: {best} Saving actor-crit...\n#################################')
                    torch.save(actor_crit.state_dict(), 'actor-crit-checkpoint')

            print('Loading best result')
            actor_crit.load_state_dict(torch.load('actor-crit-checkpoint'))
    finally:
        print('Loading best result')
        actor_crit.load_state_dict(torch.load('actor-crit-checkpoint'))


# Eval function (for evaluating the model performance)
def eval_actor(actor_crit, env):
    obs, _ = env.reset()
    total_reward = 0
    done = False
    while not done:
        action = np.argmax(actor_crit.actor(torch.tensor(obs, dtype=torch.float32)[None, :]).detach().numpy())
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        done = terminated or truncated
    return total_reward


# Rollout function (to collect data)
def rollout(actor_crit, env, N_rollout=100):
    states, actions, rewards, next_states, dones = [], [], [], [], []
    obs, info = env.reset()

    for _ in range(N_rollout):
        probs = actor_crit.actor(torch.tensor(obs, dtype=torch.float32)[None, :])[0].detach().numpy()
        action = np.random.choice(env.action_space.n, p=probs)
        states.append(obs)
        actions.append(action)

        obs_next, reward, terminated, truncated, info = env.step(action)

        rewards.append(reward)
        next_states.append(obs_next)
        dones.append(terminated or truncated)

        if terminated or truncated:
            obs, info = env.reset()
        else:
            obs = obs_next

    return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)


# Run simulation (visualize the policy)
def show(actor_crit, env):
    pi = lambda x: actor_crit.actor(torch.tensor(x[None, :], dtype=torch.float32))[0].numpy()
    with torch.no_grad():
        try:
            obs, info = env.reset()
            env.render()
            time.sleep(1)
            while True:
                action = np.argmax(pi(obs))
                obs, reward, terminated, truncated, info = env.step(action)
                env.render()
                if terminated or truncated:
                    time.sleep(0.5)
                    break
        finally:
            env.close()


if __name__ == '__main__':
    env_name = 'UnbalancedDisk'
    env = UnbalancedDisk()
    actor_crit = ActorCritic(env)

    # Define the optimizer
    optimizer = optim.Adam(actor_crit.parameters(), lr=1e-3)

    # Train the A2C model
    A2C_rollout(actor_crit, optimizer, env)

    # Show the trained policy
    show(actor_crit, env)
